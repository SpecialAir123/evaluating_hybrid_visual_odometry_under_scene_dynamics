ğŸ“Œ Evaluating Hybrid Visual Odometry Under Scene Dynamics

Hybrid Visual Odometry (VO) combines classical geometry-based pipelines with modern deep feature detectors and matchers. While classical VO (e.g., ORB-SLAM-style pipelines) performs well in static environments, it suffers when dynamic elements dominate the scene. Deep neural components such as SuperPoint, DISK, SuperGlue, and LightGlue improve feature robustness but often overfit to training domains.

This project systematically benchmarks classical, deep, and hybrid VO pipelines under varying levels of scene dynamics, using both indoor and outdoor datasets.

ğŸš€ Project Goals

Evaluate different VO pipelines (classical, deep, hybrid) under controlled scene dynamics.

Compare detectors: ORB, SuperPoint, DISK.

Compare matchers: kNN, SuperGlue, LightGlue.

Assess dynamic-object masking: optical flow (classical) vs Fast-SCNN (deep).

Quantify robustness using ATE, RPE, inlier ratio, match count, and runtime.

Ultimately, we aim to answer:
ğŸ‘‰ How do hybrid VO systems behave as scene dynamics increase, and which components contribute most to robustness?

ğŸ“ Repository Structure
evaluating_hybrid_visual_odometry_under_scene_dynamics/
 â”œâ”€â”€ detectors/
 â”‚    â”œâ”€â”€ orb_detector.py
 â”‚    â”œâ”€â”€ superpoint_infer.py
 â”‚    â””â”€â”€ disk_infer.py
 â”œâ”€â”€ matchers/
 â”‚    â”œâ”€â”€ knn_matcher.py
 â”‚    â”œâ”€â”€ superglue_infer.py
 â”‚    â””â”€â”€ lightglue_infer.py
 â”œâ”€â”€ masking/
 â”‚    â”œâ”€â”€ opticalflow_mask.py
 â”‚    â””â”€â”€ fastscnn_infer.py
 â”œâ”€â”€ geometry/
 â”‚    â””â”€â”€ pose_estimation.py
 â”œâ”€â”€ eval/
 â”‚    â”œâ”€â”€ metrics.py        # ATE, RPE, drift, inlier stats
 â”‚    â”œâ”€â”€ align.py
 â”‚    â””â”€â”€ plots.py
 â”œâ”€â”€ config/
 â”‚    â””â”€â”€ pipeline.yaml     # configure detector, matcher, mask, scenario
 â”œâ”€â”€ scripts/
 â”‚    â””â”€â”€ download_data.sh  # optional dataset downloader
 â”œâ”€â”€ main.py                # main evaluation runner
 â”œâ”€â”€ requirements.txt
 â””â”€â”€ README.md

ğŸ“¦ Installation
1. Create environment
conda create -n vo-benchmark python=3.9
conda activate vo-benchmark

2. Install dependencies
pip install -r requirements.txt

3. (Optional) Install EVO for trajectory evaluation
pip install evo --upgrade

ğŸ“Š Datasets

We evaluate VO pipelines on indoor and outdoor benchmarks covering static to highly dynamic scenes.

TUM RGB-D (Indoor)

fr1/desk â€” static

fr3/walking_xyz â€” low dynamics

fr3/walking_halfsphere â€” high dynamics

KITTI Odometry (Outdoor)

00 â€” mostly static

05 â€” medium dynamics

09 â€” high dynamics with dense traffic

Place them under data/ in this structure:

data/
 â”œâ”€â”€ TUM/fr3/walking_halfsphere/
 â””â”€â”€ KITTI/09/

ğŸ§  VO Pipelines Evaluated
Classical

ORB detector

kNN matching

Essential matrix + RANSAC

No loop closure (for fairness)

Deep Components

SuperPoint, DISK (detectors & descriptors)

SuperGlue, LightGlue (learned matchers)

Fast-SCNN for dynamic object masking

Hybrid

Classical geometry (5-point RANSAC)

Deep detector + matcher

Optional dynamic masking

ğŸ“ˆ Metrics

We evaluate each pipeline using:

Absolute Trajectory Error (ATE)

Relative Pose Error (RPE)

Inlier ratio / number of matches

Tracking failures

Runtime (FPS)

Trajectory alignment uses the Umeyama alignment (via EVO).

ğŸ§ª Running Experiments

Run a full VO experiment using:

python main.py --config config/pipeline.yaml


Example pipeline.yaml:

detector: superpoint
matcher: lightglue
masking: fastscnn
dataset: TUM
sequence: fr3/walking_halfsphere

ğŸ“Œ Expected Findings

Based on prior research and early observations:

Deep matchers (SuperGlue / LightGlue) improve robustness under moderate dynamics.

Hybrid pipelines (SuperPoint + LightGlue) offer the best balance of accuracy and runtime.

Dynamic-region masking helps significantly in highly dynamic scenes.

Classical pipelines perform well in static scenes but degrade quickly as motion increases.

ğŸ‘¥ Authors

Hongyuan Kang

Zhengbin Lu

Hanzhi Bian

Yujia Zhai

Columbia University â€” COMS 4776 (Fall 2025)