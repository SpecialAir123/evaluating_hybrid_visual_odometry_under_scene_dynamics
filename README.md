# ğŸ“Œ Evaluating Hybrid Visual Odometry Under Scene Dynamics

Hybrid Visual Odometry (VO) combines classical geometry-based pipelines with modern deep feature detectors and matchers. While classical VO (e.g., ORB-SLAM-style pipelines) performs well in static environments, it suffers when dynamic elements dominate the scene. Deep neural components such as SuperPoint, DISK, SuperGlue, and LightGlue improve feature robustness but often overfit to training domains.

This project systematically benchmarks **classical, deep, and hybrid VO pipelines** under **varying levels of scene dynamics**, using both indoor and outdoor datasets.

## ğŸš€ Project Goals

- Evaluate different VO pipelines (classical, deep, hybrid) under controlled scene dynamics
- Compare detectors: **ORB**, **SuperPoint**, **DISK**
- Compare matchers: **kNN**, **SuperGlue**, **LightGlue**
- Assess dynamic-object masking: **optical flow** (classical) vs **Fast-SCNN** (deep)
- Quantify robustness using **ATE**, **RPE**, **inlier ratio**, **match count**, and **runtime**

**Research Question:**  
ğŸ‘‰ *How do hybrid VO systems behave as scene dynamics increase, and which components contribute most to robustness?*

## ğŸ“ Repository Structure

```
evaluating_hybrid_visual_odometry_under_scene_dynamics/
â”œâ”€â”€ detectors/
â”‚   â”œâ”€â”€ orb_detector.py
â”‚   â”œâ”€â”€ superpoint_infer.py
â”‚   â””â”€â”€ disk_infer.py
â”œâ”€â”€ matchers/
â”‚   â”œâ”€â”€ knn_matcher.py
â”‚   â”œâ”€â”€ superglue_infer.py
â”‚   â””â”€â”€ lightglue_infer.py
â”œâ”€â”€ masking/
â”‚   â”œâ”€â”€ opticalflow_mask.py
â”‚   â””â”€â”€ fastscnn_infer.py
â”œâ”€â”€ geometry/
â”‚   â””â”€â”€ pose_estimation.py
â”œâ”€â”€ eval/
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ align.py
â”‚   â””â”€â”€ plots.py
â”œâ”€â”€ config/
â”‚   â””â”€â”€ pipeline.yaml
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ download_data.sh
â”œâ”€â”€ main.py
â”œâ”€â”€ requirements.txt
â””â”€â”€ README.md
```

## ğŸ“¦ Installation

### 1. Create environment
```bash
conda create -n vo-benchmark python=3.9
conda activate vo-benchmark
```

### 2. Install dependencies
```bash
pip install -r requirements.txt
```

### 3. Install EVO (for trajectory evaluation)
```bash
pip install evo --upgrade
```

## ğŸ“Š Datasets

We evaluate VO pipelines on indoor and outdoor benchmarks that cover static, moderately dynamic, and highly dynamic motion.

### TUM RGB-D (Indoor)
- `fr1/desk` â€” static
- `fr3/walking_xyz` â€” low dynamics
- `fr3/walking_halfsphere` â€” high dynamics

### KITTI Odometry (Outdoor)
- `00` â€” mostly static
- `05` â€” medium dynamics
- `09` â€” high dynamics with dense traffic

**Expected directory layout:**
```
data/
â”œâ”€â”€ TUM/
â”‚   â”œâ”€â”€ fr1/desk/
â”‚   â”œâ”€â”€ fr3/walking_xyz/
â”‚   â””â”€â”€ fr3/walking_halfsphere/
â””â”€â”€ KITTI/
    â”œâ”€â”€ 00/
    â”œâ”€â”€ 05/
    â””â”€â”€ 09/
```

## ğŸ“¥ Downloading the Datasets

### TUM RGB-D
```bash
mkdir -p data/TUM
cd data/TUM

curl -O https://vision.in.tum.de/rgbd/dataset/freiburg1/rgbd_dataset_freiburg1_desk.tgz
curl -O https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_xyz.tgz
curl -O https://vision.in.tum.de/rgbd/dataset/freiburg3/rgbd_dataset_freiburg3_walking_halfsphere.tgz

# Extract
tar -xvf rgbd_dataset_freiburg1_desk.tgz
tar -xvf rgbd_dataset_freiburg3_walking_xyz.tgz
tar -xvf rgbd_dataset_freiburg3_walking_halfsphere.tgz
```

### KITTI Odometry
```bash
mkdir -p data/KITTI
cd data/KITTI

curl -O https://s3.eu-central-1.amazonaws.com/avg-kitti/data_odometry_gray.zip
unzip data_odometry_gray.zip

# Only keep sequences 00/, 05/, 09/ (delete the rest to save disk space)
```

### Automated Script
```bash
chmod +x scripts/download_data.sh
./scripts/download_data.sh
```

## ğŸ§  VO Pipelines Evaluated

### Classical
- ORB detector
- kNN matching
- Essential Matrix + RANSAC
- Monocular pipeline (no loop closure)

### Deep Components
- **Detectors:** SuperPoint, DISK (learned detectors & descriptors)
- **Matchers:** SuperGlue, LightGlue (learned matchers)
- **Masking:** Fast-SCNN (dynamic-region removal)

### Hybrid Pipeline
- Deep detector + Deep matcher
- Classical geometric pose estimation
- Optional dynamic-object masking

## ğŸ“ˆ Metrics

We measure:
- **Absolute Trajectory Error (ATE)**
- **Relative Pose Error (RPE)**
- **Scale drift**
- **Inlier ratio**
- **Number of matches**
- **Tracking failures**
- **Runtime (FPS)**

Trajectory alignment uses the Umeyama method (via EVO toolkit).

## ğŸ§ª Running Experiments

```bash
python main.py --config config/pipeline.yaml
```

**Example `pipeline.yaml`:**
```yaml
detector: superpoint
matcher: lightglue
masking: fastscnn
dataset: TUM
sequence: fr3/walking_halfsphere
```

## ğŸ“Œ Expected Findings

- Deep matchers (SuperGlue / LightGlue) improve robustness in moderately dynamic scenes
- Hybrid pipelines (SuperPoint + LightGlue) offer the best balance of robustness and runtime
- Dynamic masking significantly stabilizes pose estimation under high dynamics
- Classical ORB + kNN works well in static scenes but degrades quickly with motion and occlusion

## ğŸ‘¥ Authors

- Hongyuan Kang
- Zhengbin Lu
- Hanzhi Bian
- Yujia Zhai

Columbia University â€” COMS 4776 (Fall 2025)

## ğŸ“„ License

MIT License
